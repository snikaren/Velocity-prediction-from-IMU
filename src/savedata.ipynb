{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"From other files\"\"\"\n",
    "from RNN_model import RNN,GRURNN\n",
    "from config import *\n",
    "\n",
    "\n",
    "def data_handling(IMU,groundtruth):\n",
    "\n",
    "    IMU.drop(index = 0, inplace = True) #Because of different timestamps\n",
    "\n",
    "    #Adjust such that timestamp start at 0 and so that time-spaces are even in Input and output\n",
    "\n",
    "    IMU['t'] = (IMU['#timestamp [ns]'] - IMU['#timestamp [ns]'].iloc[0]) * 1e-9\n",
    "    groundtruth['t'] = (groundtruth['#timestamp'] - groundtruth['#timestamp'].iloc[0]) * 1e-9\n",
    "    IMU = IMU.iloc[:len(groundtruth)] #Ensure same length\n",
    "\n",
    "    IMU.drop(columns=['#timestamp [ns]','t'], inplace=True)\n",
    "\n",
    "    #Split into train and test set\n",
    "    tot_timesteps = IMU.shape[0]\n",
    "    train_size = int((1-test_ratio) * tot_timesteps)\n",
    "\n",
    "    train_IMU = IMU[0:train_size]\n",
    "    test_IMU = IMU[train_size:]\n",
    "\n",
    "    train_groundtruth = groundtruth[0:train_size]\n",
    "    test_groundtruth = groundtruth[train_size:]\n",
    "\n",
    "    #Write to tensors\n",
    "    X_np_train = train_IMU.to_numpy()\n",
    "    X_np_test = test_IMU.to_numpy()\n",
    "\n",
    "    X_train = torch.from_numpy(X_np_train).float() \n",
    "    X_test = torch.from_numpy(X_np_test).float()\n",
    "\n",
    "    groundtruth_x_vel_train = train_groundtruth[[' v_RS_R_x [m s^-1]']]\n",
    "    groundtruth_x_vel_test = test_groundtruth[[' v_RS_R_x [m s^-1]']]\n",
    "\n",
    "    Y_np_train = groundtruth_x_vel_train.to_numpy()\n",
    "    Y_np_test = groundtruth_x_vel_test.to_numpy()\n",
    "\n",
    "    Y_train = torch.from_numpy(Y_np_train).float()\n",
    "    Y_test = torch.from_numpy(Y_np_test).float()\n",
    "\n",
    "    nr_of_features = X_train.shape[1]\n",
    "    nr_of_outputs = Y_train.shape[1]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, nr_of_features, nr_of_outputs\n",
    "\n",
    "\n",
    "def Create_sliding_windows(X,Y, seq_len, pred_len):\n",
    "    tot_timesteps = len(X)\n",
    "    num_windows = tot_timesteps-seq_len-pred_len+1 \n",
    "\n",
    "    X_windows = torch.stack([X[i:i+seq_len] for i in range(num_windows)], dim = 0)   #X_windows.shape = (num_windows, seq_len, features)\n",
    "    Y_windows = torch.stack([Y[i+seq_len:i+seq_len+pred_len] for i in range(num_windows)], dim = 0)\n",
    "\n",
    "    return X_windows, Y_windows\n",
    "\n",
    "\n",
    "def RNN_training(train_dataset,validation_dataset,nr_of_features,nr_of_outputs,model_choice,epochs,batch_size,seq_len,pred_len,learning_rate,nr_of_hidden_neurons):\n",
    "\n",
    "    \"\"\"Define RNN, Loss function and optimizer\"\"\"\n",
    "    if model_choice == \"RNN\":\n",
    "        model = RNN(input_size = nr_of_features, hidden_size = nr_of_hidden_neurons, output_size = nr_of_outputs)\n",
    "    \n",
    "    elif model_choice == \"GRU\":\n",
    "        model = GRURNN(input_size = nr_of_features, hidden_size = nr_of_hidden_neurons, output_size = nr_of_outputs)\n",
    "    else:\n",
    "        raise Exception(\"No RNN model defined\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    \"\"\"Create dataset from windows\"\"\"\n",
    "    #dataset = TensorDataset(X_windows, Y_windows) # dataset[i] = (X_windows[i], Y_windows[i])\n",
    "    #data_size = len(dataset) # nr of windows\n",
    "\n",
    "    \"\"\"Create train and HOLDOUT test dataset\"\"\"\n",
    "    #n_test = int(test_ratio * data_size)\n",
    "    #_train = data_size-n_test\n",
    "\n",
    "    #train, test = random_split(dataset,[n_train,n_test]) #Do not touch the test set\n",
    "\n",
    "    \"\"\"-----Training-----\"\"\"\n",
    "\n",
    "    \"\"\"Create test and validation dataset\"\"\"\n",
    "    #n_validation = int(val_ratio * len(train))\n",
    "    #n_train_dataset = len(train)-n_validation\n",
    "\n",
    "    #train_dataset, validation_dataset = random_split(train,[n_train_dataset,n_validation])\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(epochs):\n",
    "\n",
    "        print(f\"----STARTING EPOCH: {epochs} ----\")\n",
    "\n",
    "        model.train() #training mode\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #TODO: Look at droplast()? loader is an iterable of batches\n",
    "        val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "\n",
    "        #------TRAINING-----\n",
    "        for X_batch,Y_batch in train_loader:\n",
    "\n",
    "            #X_batch: (batch_size, seq_len, 1)\n",
    "            #before computing new gradients for the current batch, you must clear the old ones:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #e.g)\n",
    "            #[\n",
    "            #[[x7_0],[x7_1],[x7_2],[x7_3],[x7_4]],\n",
    "            #[[x1_0],[x1_1],[x1_2],[x1_3],[x1_4]],\n",
    "            #[[x3_0],[x3_1],[x3_2],[x3_3],[x3_4]],\n",
    "            #]\n",
    "\n",
    "           #Get y_pred for whole batch\n",
    "            Y_pred_batch = model.forward_sequence(X_batch,pred_len)\n",
    "\n",
    "            loss = criterion(Y_pred_batch, Y_batch) #Many-many loss over the entire batch\n",
    "\n",
    "            loss.backward() # BPTT computes ∂L/∂W, ∂L/∂U, ∂L/∂V, etc\n",
    "            optimizer.step() # one call to optimizer.step() = one weight update using the current batch’s gradients\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        #------Validation-----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "\n",
    "                Y_pred = model.forward_sequence(X_batch,pred_len)\n",
    "                loss = criterion(Y_pred, Y_batch)\n",
    "                epoch_validation_loss += loss.item()\n",
    "        \n",
    "        training_loss.append(epoch_train_loss / len(train_loader))\n",
    "        validation_loss.append(epoch_validation_loss / len(val_loader))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, label = 'Training loss', color = 'blue')\n",
    "    plt.plot(validation_loss, label = 'validation loss', color = 'red')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#TODO: add this to training:\n",
    "\"\"\"df_metrics.loc[len(df_metrics)] = {\n",
    "            \"model\": model_choice,\n",
    "            \"pred_len\": pred_len,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"nr_of_hidden_neurons\": nr_of_hidden_neurons,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"training_error\": train_error,\n",
    "            \"validation_error\": val_error,\n",
    "        }\"\"\"\n",
    "\n",
    "\n",
    "def RNN_main_pipeline():\n",
    "\n",
    "    \"\"\"Structure Data\"\"\"\n",
    "    #TODO Normalize data ? \n",
    "    IMU = pd.read_csv(r\"C:\\Users\\hampu\\Desktop\\RNN_test\\Velocity-prediction-from-IMU\\Data\\IMU_data\\data.csv\")\n",
    "    groundtruth = pd.read_csv(r\"C:\\Users\\hampu\\Desktop\\RNN_test\\Velocity-prediction-from-IMU\\Data\\state_groundtruth_data\\data.csv\")\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test, nr_of_features, nr_of_outputs = data_handling(IMU,groundtruth)\n",
    "\n",
    "    #Store results in csv_file\n",
    "    header = [\"model\", \"pred_len\", \"seq_len\", \"batch_size\", \"learning_rate\", \"nr_of_hidden_neurons\", \"epoch\", \"training_error\", \"validation_error\"]\n",
    "\n",
    "    \"\"\"Set up training for various cases\"\"\"\n",
    "    for seq_len in seq_len_list:\n",
    "        for pred_len in pred_len_list:\n",
    "            X_windows, Y_windows = Create_sliding_windows(X_train,Y_train, seq_len, pred_len)\n",
    "            dataset = TensorDataset(X_windows, Y_windows) # dataset[i] = (X_windows[i], Y_windows[i])\n",
    "            data_size = len(dataset)\n",
    "            n_validation = int(val_ratio * data_size)\n",
    "            n_train = data_size-n_validation\n",
    "\n",
    "            train, validation = random_split(dataset,[n_train,n_validation]) #TODO: Is it fine that they are different splits?\n",
    "\n",
    "            for model_choice in model_choice_list:\n",
    "                for epochs in epochs_list:\n",
    "                    for batch_size in batch_size_list:\n",
    "                        for learning_rate in learning_rate_list:\n",
    "                            for nr_of_hidden_neurons in nr_of_hidden_neurons_list:\n",
    "\n",
    "                                df_metrics = pd.DataFrame(columns=header)\n",
    "                \n",
    "                                RNN_training(train,validation,nr_of_features,nr_of_outputs,model_choice,epochs,batch_size,seq_len,pred_len,learning_rate,nr_of_hidden_neurons,df_metrics)\n",
    "                                \n",
    "                                filename = (\n",
    "                                    f\"{seq_len}seqlen_{pred_len}predlen_\"\n",
    "                                    f\"{model_choice}_{epochs}epochs_\"\n",
    "                                    f\"{batch_size}batchsize_{learning_rate}learning_rate_\"\n",
    "                                    f\"{nr_of_hidden_neurons}_hiddenneurons.csv\"\n",
    "                                )\n",
    "\n",
    "                                df_metrics.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
